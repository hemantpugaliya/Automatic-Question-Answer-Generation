{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using an existing server http://localhost:9000\n",
      "INFO:root:The server is available.\n"
     ]
    }
   ],
   "source": [
    "#Implementation for the publication https://ieeexplore.ieee.org/document/7732102/\n",
    "# A few enhancements have been done to extract answers and generalize the rules based on syntax.\n",
    "import nltk\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import Tree\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk import ne_chunk\n",
    "import itertools\n",
    "import collections\n",
    "import logging\n",
    "#requires Java 1.8 or above\n",
    "#Start  a stanforrrdd CoreNLP server - used stanford-corenlp-full-2018-02-27 for development\n",
    "#java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer   -port 9000 -timeout 150000\n",
    "#import logging\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000,logging_level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting New Paper Impl - A Rule based Question Generation Framework to deal with Simple and\n",
    "#Complex Sentences\n",
    "#sentence = \"Barack Obama is the president of The United States of America.\"\n",
    "#sentence = \"IIIT Hyderabad is the venue of IASNLP-2018.\"\n",
    "#sentence = \"The boy went by bus.\"\n",
    "#sentence =  sentence.rstrip().rstrip(\".\")\n",
    "#sentence = \"The contractor will build you a house for $100,000 dollars.\"\n",
    "#sentence = \"The book might cost me $10.\"\n",
    "#sentence=\"The book might cost me $10 from the store.\"\n",
    "#sentence = \"$100,000 builds a house out of sticks.\"\n",
    "#sentence = \"The bill will cost them 500 million dollars in India.\"\n",
    "#sentence = \"His name is Robinson.\"\n",
    "#sentence = \"She will quickly pour the sticky liquid into the green flowery pot.\"\n",
    "#sentence = \"I am going quickly back on Saturday.\"\n",
    "#sentence = \"He wants to become a good doctor.\"\n",
    "#sentence = \"I want to work.\"\n",
    "#sentence = \"He hurriedly left the class in the morning.\"\n",
    "#sentence = \"He is addicted to smoking.\"\n",
    "#sentence = \"He will go by bus.\"\n",
    "#sentence = \"John gave Mary a book.\" #design more rules to catch the essence\n",
    "#sentence = \"He gave him a book.\"\n",
    "#sentence = \"He will buy a book.\"\n",
    "#sentence = \"He gave him a book.\"\n",
    "#sentence = \"John gave Mary a book.\"\n",
    "\n",
    "#print(segments)\n",
    "#dep = nlp.dependency_parse(sentence)\n",
    "#print(dep)\n",
    "#tree.draw()\n",
    "\n",
    "#print(ner)\n",
    "#pos = tree.treeposition_spanning_leaves(0,9)\n",
    "\n",
    "def get_lca_length(location1, location2):\n",
    "    i = 0\n",
    "    while i < len(location1) and i < len(location2) and location1[i] == location2[i]:\n",
    "        i+=1\n",
    "    return i\n",
    "\n",
    "def findLCA(ptree, text1, text2):\n",
    "    leaf_values = ptree.leaves()\n",
    "    leaf_index1 = leaf_values.index(text1)\n",
    "    leaf_index2 = leaf_values.index(text2)\n",
    "\n",
    "    location1 = ptree.leaf_treeposition(leaf_index1)\n",
    "    location2 = ptree.leaf_treeposition(leaf_index2)\n",
    "\n",
    "    #find length of least common ancestor (lca)\n",
    "    lca_len = get_lca_length(location1, location2)\n",
    "    return ptree[location1[:lca_len]]\n",
    "#for ptree in parse_trees :\n",
    "    #ptree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chunks(tagged_segment , grammar):\n",
    "    #print(tagged_segment)\n",
    "    grammar = r\"CHUNK: \" + grammar\n",
    "    #print(grammar)\n",
    "    cp = RegexpParser(grammar)\n",
    "    tree = cp.parse(tagged_segment)\n",
    "    return tree\n",
    "def find_chunk(chunks):\n",
    "    if not isinstance(chunks, nltk.tree.Tree):\n",
    "         return [ [subtree for subtree in chunk.subtrees(filter = lambda t: t.label() in ['CHUNK']) ] for chunk in chunks]   \n",
    "    else :\n",
    "        return [subtree for subtree in chunks.subtrees(filter = lambda t: t.label() in ['CHUNK'])]\n",
    "def is_clause(segment_chunks_tree):\n",
    "    if  not isinstance(chunks, nltk.Tree):\n",
    "         return [ not not chunk for chunk in find_chunk(chunks) ]   \n",
    "    else :\n",
    "        return not not find_chunk(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling Special case :If a segment contains only verb phrase,\n",
    "#the previous segments are also checked for the existence\n",
    "#of any subject phrase related to the verb phrase.\n",
    "\n",
    "def is_only_VP(parse_trees):\n",
    "    return [tree.label() == 'VP' for tree in parse_trees]\n",
    "\n",
    "\n",
    "def find_the_closest_NP_for_VP(parse_trees):\n",
    "    is_VP = is_only_VP(parse_trees)\n",
    "    closest_NP = []\n",
    "    for index,truth_val in enumerate(is_VP):\n",
    "        if not truth_val:\n",
    "            closest_NP.append(None)\n",
    "        else:\n",
    "            found_NP = False\n",
    "            for index_tree in reversed(range(0,index)):\n",
    "                for child in parse_trees[index_tree] :\n",
    "                    if child.label() == 'NP':\n",
    "                        closest_NP.append(child)\n",
    "                        found_NP = True\n",
    "                        break\n",
    "                if found_NP:\n",
    "                    break\n",
    "            if not found_NP :\n",
    "                closest_NP.append(None)\n",
    "    return closest_NP\n",
    "def enrich_VPs(parse_trees):\n",
    "    enrichment_data = find_the_closest_NP_for_VP(parse_trees)\n",
    "    enriched_parse_trees =  []\n",
    "    enrichment_done =[]\n",
    "    for ptree,enrich in zip(parse_trees,enrichment_data):\n",
    "        if enrich :\n",
    "            enriched_parse_trees.append(Tree('S', [enrich.copy(deep=True),ptree]))\n",
    "        else:\n",
    "            enriched_parse_trees.append(ptree)\n",
    "    return enriched_parse_trees , [ not not data for data in enrichment_data ]\n",
    "\n",
    "\n",
    "\n",
    "def find_VP_tree(parse_tree):\n",
    "    for child  in parse_tree :\n",
    "        #print (child.label())\n",
    "        if child.label() == \"VP\":\n",
    "            return child\n",
    "def find_NP_tree(parse_tree):\n",
    "    for child  in parse_tree :\n",
    "        #print (child.label())\n",
    "        if child.label() == \"NP\":\n",
    "            return child\n",
    "\n",
    "def chunk_VP_NP_parts(parse_tree,chunk_tree) :\n",
    "    NP = find_NP_tree(parse_tree).leaves()\n",
    "    VP = find_VP_tree(parse_tree).leaves()\n",
    "    #print(NP,VP)\n",
    "    NP_POS = []\n",
    "    VP_POS = []\n",
    "    #print(chunk_tree)\n",
    "    chunk_pos = chunk_tree.pos()\n",
    "    #print(chunk_pos)\n",
    "    #print(chunk_pos)\n",
    "    for pos in chunk_pos :\n",
    "        if pos[0][0] in NP :\n",
    "            NP.remove(pos[0][0])\n",
    "            NP_POS.append(pos[0])\n",
    "        elif pos[0][0] in VP:\n",
    "            VP.remove(pos[0][0])\n",
    "            VP_POS.append(pos[0])\n",
    "    #print(NP_POS,VP_POS)\n",
    "    return NP_POS,VP_POS\n",
    "\n",
    "def verb_phrase_identification(parse_trees,is_clause,chunks):\n",
    "    verb_phrase = [] \n",
    "    for tree,chunk,is_clause in zip(parse_trees,chunks,is_clause) :\n",
    "        if is_clause :\n",
    "            #print(tree)\n",
    "            #print(type(chunk))\n",
    "            chunk_tree = find_chunk(chunk)[0]\n",
    "            #print(chunk_tree)\n",
    "            #print(tree)\n",
    "            NP_POS,VP_POS = chunk_VP_NP_parts(tree,chunk_tree)\n",
    "            #print(VP_POS)       \n",
    "            if len(VP_POS) > 1 :\n",
    "                verb_phrase.append(VP_POS[0][0])\n",
    "            else :\n",
    "                vp_tag = VP_POS[0][1]\n",
    "                if vp_tag == \"VBD\" :\n",
    "                    verb_phrase.append(\"did\")\n",
    "                elif vp_tag == \"VBP\" or vp_tag == \"VB\" :\n",
    "                    verb_phrase.append(\"do\")\n",
    "                elif vp_tag == \"VBZ\" :\n",
    "                    verb_phrase.append(\"does\")\n",
    "                else :\n",
    "                    verb_phrase.append(None)\n",
    "        else :\n",
    "            verb_phrase.append(None)\n",
    "    return verb_phrase\n",
    "            \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_subj(parse_tree):\n",
    "    for child  in parse_tree :\n",
    "        #print (child.label())\n",
    "        if child.label() == \"NP\":\n",
    "            return child.leaves()\n",
    "    return []\n",
    "def find_VP(parse_tree):\n",
    "    for child  in parse_tree :\n",
    "        #print (child.label())\n",
    "        if child.label() == \"VP\":\n",
    "            return child.leaves()\n",
    "    return []\n",
    "def QSG_Rule_6_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<IN>+<\\$>*<CD>+}\"\n",
    "        #print(rule_grammar)\n",
    "        seg_pos = parse_tree.pos()\n",
    "        #print(seg_pos)\n",
    "        rule6_1_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        #print(rule6_1_chunks)\n",
    "        prep_chunk = find_chunk(rule6_1_chunks)\n",
    "        #print(prep_chunk)\n",
    "        if prep_chunk :\n",
    "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
    "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
    "            prep_part_tokens = [ p[0] for p in find_chunk(parse_chunks(prep_pos , \"{<IN>+}\" ))[0].leaves()]\n",
    "            #print(prep_part_tokens)\n",
    "            answer_words = [x for x in prep_tokens if x not in prep_part_tokens ] \n",
    "            #print(answer)\n",
    "            subject = find_subj(parse_tree)\n",
    "            VP = find_VP(parse_tree)\n",
    "            rem_verb_phrase =  verb if verb in VP else VP[0][0]\n",
    "            VP = VP[1:]\n",
    "            \n",
    "            #Calc rest of VP for Question Generation\n",
    "            [VP.remove(x) for x in prep_part_tokens + answer_words if x in VP]\n",
    "            #calc rest of the sentence for question generation\n",
    "            answer = \" \".join(answer_words)\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in subject + VP + prep_part_tokens + answer_words + [rem_verb_phrase] if x in tok ]\n",
    "            quest_tok = prep_part_tokens + [\"how\",\"much\"]+ [verb] + subject + VP + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : answer })\n",
    "            \n",
    "    return QA\n",
    "\n",
    "def QSG_Rule_6_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "        rule_grammar = \"{<\\$>*<CD>+<MD>?<VB|VBD|VBG|VBP|VBN|VBZ|IN>+}\"\n",
    "        #print(rule_grammar)\n",
    "        seg_pos = parse_tree.pos()\n",
    "        #print(seg_pos)\n",
    "        rule6_2_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        #print(rule6_2_chunks)\n",
    "        rule_chunk = find_chunk(rule6_2_chunks)\n",
    "        #print(rule_chunk)\n",
    "        if rule_chunk :\n",
    "            rule_pos =  [ pos[0] for pos in rule_chunk[0].pos() ]\n",
    "            rule_tokens = [ pos[0][0] for pos in rule_chunk[0].pos() ]\n",
    "            prep_part_tokens = [ p[0] for p in find_chunk(parse_chunks(rule_pos , \"{<\\$>*<CD>+}\" ))[0].leaves()]\n",
    "            #print(prep_part_tokens)\n",
    "            answer_words = prep_part_tokens\n",
    "            [rule_tokens.remove(x) for x in answer_words if x in rule_tokens ]\n",
    "            #calc rest of the sentence for question generation\n",
    "            answer = \" \".join(answer_words)\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in answer_words + rule_tokens ]\n",
    "            #print(answer)\n",
    "            quest_tok = [\"how\",\"much\"]+ rule_tokens + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : answer })\n",
    "    return QA\n",
    "\n",
    "def QSG_Rule_6_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<MD>?<VB|VBD|VBG|VBP|VBN|VBZ>+<IN>?<NN|NNS|NNP|NNPS|PRP|PRP\\$>?<\\$>*<CD>+}\"\n",
    "        #print(rule_grammar)\n",
    "        seg_pos = parse_tree.pos()\n",
    "        #print(seg_pos)\n",
    "        rule6_3_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        #print(rule6_1_chunks)\n",
    "        prep_chunk = find_chunk(rule6_3_chunks)\n",
    "        #print(prep_chunk)\n",
    "        if prep_chunk :\n",
    "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
    "            #print(prep_pos)\n",
    "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
    "            ans_tokens = [p[0]for p in find_chunk(parse_chunks(prep_pos , \"{<\\$>*<CD>+}\"))[0].leaves()]\n",
    "            \n",
    "            #print(prep_part_tokens)\n",
    "            VP = find_VP(parse_tree)\n",
    "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
    "            [prep_tokens.remove(x) for x in ans_tokens + [rem_verb_phrase] if  x in prep_tokens ]\n",
    "            #print(verb)\n",
    "            #print(prep_tokens)\n",
    "            subject = find_subj(parse_tree)\n",
    "            #print(subject)\n",
    "            \n",
    "            #Calc rest of VP for Question Generation\n",
    "            [VP.remove(x) for x in prep_tokens + ans_tokens if x in VP]\n",
    "            #calc rest of the sentence for question generation\n",
    "            answer = \" \".join(ans_tokens)\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in subject + VP + prep_tokens + ans_tokens + [rem_verb_phrase] if x in tok ]\n",
    "            quest_tok =  [\"how\",\"much\"]+ [verb] + subject + prep_tokens + VP + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : answer })\n",
    "    return QA\n",
    "                \n",
    "                \n",
    "\n",
    "#print(QSG_Rule_6_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "#print(QSG_Rule_6_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "#print(QSG_Rule_6_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ner_tags_for_pos (ners,pos):\n",
    "    return list(filter(lambda x : x[0] in [p[0] for p in pos] , ners))\n",
    "\n",
    "def QSG_Rule_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok in zip(chunks , parse_trees , is_clause_val, ner_split,tokens) :\n",
    "        if is_cl :\n",
    "            chunk_pos = [ pos[0] for pos in find_chunk(chunk)[0].pos() ]\n",
    "            grammar = \"{<DT>?<JJ.?>*<NN.?|PRP|PRP$|POS|IN|DT|CC|VBG|VBN>+}\"\n",
    "            rule1_chunks = parse_chunks(chunk_pos,grammar)\n",
    "            noun_chunk = find_chunk(rule1_chunks)\n",
    "            if noun_chunk :\n",
    "                noun_pos = noun_chunk[0].leaves()\n",
    "                #print(ner)\n",
    "                #print(noun_pos)\n",
    "                tok = tok[:]\n",
    "                ner_tags = find_ner_tags_for_pos(ner,noun_pos)\n",
    "                qsd4,q_disambg = QSD_Rule_4(ner_tags,\"QSG_RULE_1\")\n",
    "                answer_words = [ p[0] for p in noun_pos ]\n",
    "                #print(tok)\n",
    "                #print(answer_words)\n",
    "                [ tok.remove(ans) for ans in answer_words if ans in tok]\n",
    "                answer = \" \".join(answer_words)\n",
    "                #print(tok)\n",
    "                quest_tok = [q_disambg] + tok + [\"?\"]\n",
    "                #print(quest_tok)\n",
    "                question = \" \".join(quest_tok)\n",
    "                QA.append({\"Q\" : question , \"A\" : answer })\n",
    "    return QA\n",
    "            \n",
    "#QSG_Rule_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSG_Rule_7(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<DT>?<CD>+<RB>?<JJ|JJR|JJS>?<NN|NNS|NNP|NNPS|VBG>+}\"\n",
    "        #print(rule_grammar)\n",
    "        seg_pos = parse_tree.pos()\n",
    "        #print(seg_pos)\n",
    "        rule7_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        #print(rule7_chunks)\n",
    "        prep_chunk = find_chunk(rule7_chunks)\n",
    "        #print(prep_chunk)\n",
    "        if prep_chunk :\n",
    "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
    "            #print(prep_pos)\n",
    "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
    "            ans_tokens = [p[0]for p in find_chunk(parse_chunks(prep_pos , \"{<CD>+}\"))[0].leaves()]\n",
    "            \n",
    "            #print(prep_part_tokens)\n",
    "            VP = find_VP(parse_tree)\n",
    "            if not VP :\n",
    "                break\n",
    "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
    "            [prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
    "            #print(verb)\n",
    "            #print(prep_tokens)\n",
    "            subject = find_subj(parse_tree)\n",
    "            #print(subject)\n",
    "            \n",
    "            #Calc rest of VP for Question Generation\n",
    "            [VP.remove(x) for x in prep_tokens + ans_tokens + [rem_verb_phrase] if x in VP]\n",
    "            #calc rest of the sentence for question generation\n",
    "            answer = \" \".join(ans_tokens)\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in subject + VP + prep_tokens + ans_tokens + [rem_verb_phrase] if x in tok ]\n",
    "            quest_tok =  [\"how\",\"many\"]+ prep_tokens + [verb] + subject + VP + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : answer })\n",
    "    return QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSG_Rule_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<PRP\\$|POS>+<RB.?>*<JJ.?>*<NN.?|VBG|VBN>+<VB.?|MD|RP>+}\"\n",
    "        #print(rule_grammar)\n",
    "        seg_pos = parse_tree.pos()\n",
    "        #print(seg_pos)\n",
    "        rule3_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        #print(rule3_chunks)\n",
    "        prep_chunk = find_chunk(rule3_chunks)\n",
    "        #print(prep_chunk)\n",
    "        if prep_chunk :\n",
    "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
    "            #print(prep_pos)\n",
    "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
    "            ans_tokens = [p[0]for p in find_chunk(parse_chunks(prep_pos , \"{<PRP\\$|POS>+}\"))[0].leaves()]\n",
    "            \n",
    "            #print(prep_part_tokens)\n",
    "            [prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
    "            #print(verb)\n",
    "            #print(prep_tokens)\n",
    "            #subject = find_subj(parse_tree)\n",
    "            #print(subject)\n",
    "            \n",
    "            #Calc rest of VP for Question Generation\n",
    "            #[VP.remove(x) for x in prep_tokens + ans_tokens if x in tok]\n",
    "            #calc rest of the sentence for question generation\n",
    "            answer = \" \".join(ans_tokens)\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in prep_tokens + ans_tokens  if x in tok ]\n",
    "            quest_tok =  [\"Whose\"]+ prep_tokens + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : answer })\n",
    "    return QA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSG_Rule_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<DT>?<JJ.?>?<RB>?<IN|TO|RP>+<DT>*<JJ.?>*<NN.?|PP|PRP|PRP\\$ >+<VBG|POS|CD|RB|DT>*}\"\n",
    "        #print(rule_grammar)\n",
    "        seg_pos = parse_tree.pos()\n",
    "        #print(seg_pos)\n",
    "        rule4_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        #print(rule4_chunks)\n",
    "        prep_chunk = find_chunk(rule4_chunks)\n",
    "        #print(prep_chunk)\n",
    "        if prep_chunk :\n",
    "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
    "            #print(prep_pos)\n",
    "            ans_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
    "            prep_tokens = [p[0]for p in find_chunk(parse_chunks(prep_pos , \"{<IN>+}\"))[0].leaves()] if find_chunk(parse_chunks(prep_pos , \"{<IN>+}\")) else None\n",
    "            #print(prep_tokens)\n",
    "            if not prep_tokens  or  not any([x in [\"under\", \"across\", \"around\", \"along\", \"through\", \"over\", \"into\", \"onto\"] for x in prep_tokens]):\n",
    "                break;\n",
    "            \n",
    "            VP = find_VP(parse_tree)\n",
    "            if not VP :\n",
    "                break\n",
    "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
    "            \n",
    "            #print(prep_tokens)\n",
    "            #print(VP)\n",
    "            #print(verb)\n",
    "            #print(prep_tokens)\n",
    "            subject = find_subj(parse_tree)\n",
    "            #print(subject)\n",
    "            answer = \" \".join(ans_tokens)\n",
    "            #Calc rest of VP for Question Generation\n",
    "            #[VP.remove(x) for x in ans_tokens + [rem_verb_phrase] if x in VP]\n",
    "            #calc rest of the sentence for question generation\n",
    "            VP = \" \".join(VP).replace(answer , \"\").split(\" \")\n",
    "            VP.remove(rem_verb_phrase)\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in ans_tokens + subject + VP + [rem_verb_phrase] if x in tok ]\n",
    "            #print(tok)\n",
    "            quest_tok =  [\"Where\"]+ [verb] + subject + VP + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : answer })\n",
    "    return QA\n",
    "\n",
    "#QSG_Rule_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSG_Rule_5(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<DT>?<JJ.?>?<RB>?<IN|TO|RP>+<DT>*<NN.?>+}\"\n",
    "        #print(rule_grammar)\n",
    "        seg_pos = parse_tree.pos()\n",
    "        #print(seg_pos)\n",
    "        rule5_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        #print(rule5_chunks)\n",
    "        prep_chunk = find_chunk(rule5_chunks)\n",
    "        #print(prep_chunk)\n",
    "        if prep_chunk :\n",
    "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
    "            #print(prep_pos)\n",
    "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
    "            ans_pos = [p for p in find_chunk(parse_chunks(prep_pos , \"{<IN|TO|RP>+<DT>*<NN.?>+}\"))[0].leaves()]\n",
    "            ans_tokens = [ pos[0] for pos in ans_pos ]\n",
    "            [prep_tokens.remove(x) for x in ans_tokens]\n",
    "            #print(prep_tokens)\n",
    "            ans_ner = [ p[1] for p in find_ner_tags_for_pos(ner,ans_pos) ]\n",
    "            not_date_time_ner = not(\"DATE\" in ans_ner or \"TIME\" in ans_ner )\n",
    "            #print(ans_ner)\n",
    "            if not_date_time_ner and (not any([x.lower() in [\"tomorrow\",\"yesterday\", \"today\", \"tonight\", \"am\", \"pm\"] for x in ans_tokens])):\n",
    "                break;\n",
    "            \n",
    "            VP = find_VP(parse_tree)\n",
    "            if not VP :\n",
    "                break\n",
    "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
    "            \n",
    "            #print(prep_tokens)\n",
    "            #print(VP)\n",
    "            #print(verb)\n",
    "            #print(prep_tokens)\n",
    "            subject = find_subj(parse_tree)\n",
    "            #print(subject)\n",
    "            answer = \" \".join(ans_tokens)\n",
    "            #Calc rest of VP for Question Generation\n",
    "            #[VP.remove(x) for x in ans_tokens + [rem_verb_phrase] if x in VP]\n",
    "            #calc rest of the sentence for question generation\n",
    "            VP = \" \".join(VP).replace(answer , \"\").split(\" \")\n",
    "            VP.remove(rem_verb_phrase)\n",
    "            [VP.remove(x) for x in prep_tokens ]\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in ans_tokens + subject + VP + [rem_verb_phrase] + prep_tokens if x in tok ]\n",
    "            #print(tok)\n",
    "            quest_tok =  [\"When\"]+ [verb] + subject + VP + prep_tokens + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : answer })\n",
    "    return QA\n",
    "\n",
    "#QSG_Rule_5(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSG_Rule_2_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<TO>+<VB|VBP|RP>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT>*}\"\n",
    "        #print(rule_grammar)\n",
    "        seg_pos = parse_tree.pos()\n",
    "        #print(seg_pos)\n",
    "        rule2_4_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        #print(rule2_4_chunks)\n",
    "        prep_chunk = find_chunk(rule2_4_chunks)\n",
    "        #print(prep_chunk)\n",
    "        if prep_chunk :\n",
    "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
    "            #print(prep_pos)\n",
    "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
    "            ans_chunk = find_chunk(parse_chunks(prep_pos , \"{<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT>*}\"))\n",
    "            ans_NP = None\n",
    "            if ans_chunk :\n",
    "                ans_NP = True\n",
    "                rep_chunk = find_chunk(parse_chunks(prep_pos , \"{<TO>+<VB|VBP|RP>+}\"))\n",
    "                \n",
    "            else :\n",
    "                ans_chunk = find_chunk(parse_chunks(prep_pos , \"{<TO>+<VB|VBP|RP>+}\"))\n",
    "                ans_NP = False\n",
    "            \n",
    "            ans_pos = [p for p in ans_chunk[0].leaves()]\n",
    "            ans_tokens = [ pos[0] for pos in ans_pos ]\n",
    "            #print(ans_pos)\n",
    "            #print(ans_tokens)\n",
    "            if ans_NP :\n",
    "                rep_pos = [p for p in rep_chunk[0].leaves()]\n",
    "                rep_tokens = [ pos[0] for pos in rep_pos ]\n",
    "            else:\n",
    "                rep_tokens = [\"to\" , \"do\"]\n",
    "            \n",
    "            prep = \" \".join(prep_tokens)\n",
    "            rep = \" \".join(rep_tokens)\n",
    "            #print(prep)\n",
    "            \n",
    "            #print(rep)\n",
    "            rem_VP = rep\n",
    "            #print(rem_VP)\n",
    "            #print(rep_tokens)\n",
    "            #rem_VP = \" \".join(prep_tokens).replace(\" \".join(ans_tokens),\" \".join(rep_tokens))\n",
    "            #print(rem_VP)\n",
    "            [prep_tokens.remove(x) for x in ans_tokens]\n",
    "            #print(prep_tokens)\n",
    "            #ans_ner = [ p[1] for p in find_ner_tags_for_pos(ner,ans_pos) ]\n",
    "            #not_date_time_ner = not(\"DATE\" in ans_ner or \"TIME\" in ans_ner )\n",
    "            #print(ans_ner)\n",
    "            #if not_date_time_ner and (not any([x.lower() in [\"tomorrow\",\"yesterday\", \"today\", \"tonight\", \"am\", \"pm\"] for x in ans_tokens])):\n",
    "                #break;\n",
    "            \n",
    "            VP = find_VP(parse_tree)\n",
    "            if not VP :\n",
    "                break\n",
    "            \n",
    "            #print(prep_tokens)\n",
    "            #print(VP)\n",
    "            #print(verb)\n",
    "            #print(prep_tokens)\n",
    "            subject = find_subj(parse_tree)\n",
    "            #print(subject)\n",
    "            answer = \" \".join(ans_tokens)\n",
    "            #Calc rest of VP for Question Generation\n",
    "            [VP.remove(x) for x in ans_tokens + rem_VP.split(\" \") if x in VP]\n",
    "            #calc rest of the sentence for question generation\n",
    "            #VP = \" \".join(VP).replace(answer , \"\").split(\" \")\n",
    "            #VP.remove(rem_verb_phrase)\n",
    "            #[VP.remove(x) for x in prep_tokens ]\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in ans_tokens + subject + VP + rem_VP.split(\" \") + ans_tokens if x in tok ]\n",
    "            #print(tok)\n",
    "            quest_tok =  [\"What\"]+ [verb] + subject + VP + rem_VP.split(\" \") + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : answer })\n",
    "    return QA\n",
    "\n",
    "#QSG_Rule_2_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing here the implementation of QSD rule befor implementing QSG 2.1 - 2.4 \n",
    "def find_ner_tag_for_token(ners,token):\n",
    "    for tag in ner :\n",
    "        if tag[0] == token :\n",
    "            return tag[1]\n",
    "    return None\n",
    "\n",
    "def find_ner_tag_for_tokens(ners,tokens):\n",
    "    return list(filter(lambda x : x[0] in tokens , ners))\n",
    "\n",
    "def get_pos_tokens_from_chunk_tree(chunk_tree):\n",
    "    rep_pos = [p for p in chunk_tree.leaves()] if chunk_tree else []\n",
    "    rep_tokens = [ pos[0] for pos in rep_pos ] if chunk_tree else []\n",
    "    return rep_pos,rep_tokens\n",
    "def QSD_Rule_1(chunk_pos) :\n",
    "    #print(ner_chunk_tags)\n",
    "    disambg_value =  all( [x == \"PRP\" for x in [p[1] for p in chunk_pos]])\n",
    "    if disambg_value :\n",
    "        return disambg_value,\"whom\"\n",
    "    else:\n",
    "        return disambg_value,\"what\"\n",
    "def QSD_Rule_2(parse_tree):\n",
    "    VP = None\n",
    "    #parse_tree.draw()\n",
    "    for child  in parse_tree :\n",
    "        #print (child.label())\n",
    "        if child.label() == \"VP\":\n",
    "            VP =  child\n",
    "    if VP :\n",
    "        NP_in_VP = []\n",
    "        for child  in VP :\n",
    "            if child.label() == \"NP\":\n",
    "                NP_in_VP.append(child)\n",
    "        #print(NP_in_VP)\n",
    "        if len(NP_in_VP) > 1 :\n",
    "            return True , NP_in_VP[0].leaves() , NP_in_VP[1].leaves()\n",
    "        else :\n",
    "            return False , [] , NP_in_VP[0].leaves() if NP_in_VP else []\n",
    "    else :\n",
    "        return False , [] , []\n",
    "\n",
    "def QSD_Rule_3(chunk_pos,chunk_ners) :\n",
    "    first_noun_chunk  = find_chunk(parse_chunks(chunk_pos , \"{<NN.?>+}\"))\n",
    "    if first_noun_chunk :\n",
    "        first_noun_pos, first_noun_tokens = get_pos_tokens_from_chunk_tree(first_noun_chunk[0])\n",
    "        if find_ner_tag_for_token(chunk_ners,first_noun_tokens[0]) == \"PERSON\":\n",
    "            return True,\"Whom\"\n",
    "        else:\n",
    "            return False,\"What\"\n",
    "    else: \n",
    "        return False,\"What\"\n",
    "\n",
    "    \n",
    "        \n",
    "def QSD_Rule_4(ner_chunk_tags,QSG_rule) :\n",
    "    #print(ner_chunk_tags)\n",
    "    disambg_value =  ner_chunk_tags[0][1] in ['LOCATION','ORGANIZATION', 'CITY','COUNTRY']\n",
    "    if QSG_rule == \"QSG_RULE_1\" :\n",
    "        if disambg_value :\n",
    "            return disambg_value,\"what\"\n",
    "        elif ner_chunk_tags[0][1] in ['PERSON']:\n",
    "            return disambg_value,\"who\"\n",
    "        else:\n",
    "            return disambg_value,\"who\"\n",
    "    if QSG_rule == \"QSG_RULE_2_1\":\n",
    "        if disambg_value :\n",
    "            return disambg_value,\"where\"\n",
    "        else :\n",
    "            return disambg_value,\"To what\"\n",
    "    if QSG_rule == \"QSG_RULE_2_2\":\n",
    "        if disambg_value :\n",
    "            return disambg_value,\"where\"\n",
    "        else :\n",
    "            return disambg_value,\"what\"\n",
    "\n",
    "def QSD_Rule_5(chunk_pos,chunk_ners) :\n",
    "    noun_chunk  = find_chunk(parse_chunks(chunk_pos , \"{<NN.?>+}\"))\n",
    "    if noun_chunk :\n",
    "        noun_pos, noun_tokens = get_pos_tokens_from_chunk_tree(noun_chunk[0])\n",
    "        noun_ners = find_ner_tag_for_tokens(chunk_ners,noun_tokens)\n",
    "        ners = set([ x[1] for x in noun_ners])\n",
    "        if \"TIME\" in ners or \"DATE\" in ners :\n",
    "            return \"when\"\n",
    "        else:\n",
    "            return \"what\"\n",
    "    else: \n",
    "        return \"What\"\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSG_Rule_2_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<TO>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|VBG|DT|POS|CD|VBN>+}\"\n",
    "        seg_pos = parse_tree.pos()\n",
    "        rule2_1_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        seg = \" \".join([p[0] for p in seg_pos ])\n",
    "        #print(seg)\n",
    "        #print(rule2_1_chunks)\n",
    "        prep_chunk = find_chunk(rule2_1_chunks)\n",
    "        prep_pos,prep_tokens = get_pos_tokens_from_chunk_tree(prep_chunk[0]) if prep_chunk else (None,None)\n",
    "        #print(prep_pos)\n",
    "        #print(prep_tokens)\n",
    "        prep = \" \".join(prep_tokens) if prep_tokens else []\n",
    "        clause_chunk  = find_chunk(chunk)\n",
    "        #print(clause_chunk)\n",
    "        if len(clause_chunk) > 1 :\n",
    "            clause_strings = [\" \".join(get_pos_tokens_from_chunk_tree(c)[1]) for c in clause_chunk ]\n",
    "            prep_index = seg.index(seg)\n",
    "            clause_index  = [abs(seg.index(cs)-prep_index)  for cs in clause_strings ]\n",
    "            cl_chunk = clause_chunk[clause_index.index(min(clause_index))]\n",
    "            cl_string = \" \".split(clause_strings[clause_index.index(min(clause_index))])\n",
    "            #print(cl_string)\n",
    "            verb = verb_phrase_identification([findLCA(parse_tree,cl_string[0],cl_string[-1])],[True],[cl_chunk])\n",
    "            #print(verb)\n",
    "        else :\n",
    "            cl_chunk = clause_chunk[0] if clause_chunk else None\n",
    "        cl_pos,cl_tokens = get_pos_tokens_from_chunk_tree(cl_chunk)\n",
    "        #print(seg_pos)\n",
    "        #print(prep_chunk)\n",
    "        #print(cl_chunk)\n",
    "        if prep_chunk :\n",
    "            \n",
    "            ques = \"To what\"\n",
    "            qsd1,ques = QSD_Rule_1(prep_pos)\n",
    "            if qsd1 :\n",
    "                ques = \"To \" + ques\n",
    "            else :\n",
    "                prep_ners = find_ner_tag_for_tokens(ner,prep_tokens)\n",
    "                qsd3,ques = QSD_Rule_3(prep_pos,prep)\n",
    "                ques = \"To \" + ques\n",
    "                if not qsd3 :\n",
    "                    qsd4,ques = QSD_Rule_4(prep_ners,\"QSG_RULE_2_1\")\n",
    "                \n",
    "            #print(ques)\n",
    "            #print(verb)\n",
    "                \n",
    "            VP = find_VP(parse_tree)\n",
    "            #print(parse_tree)\n",
    "            if not VP :\n",
    "                break\n",
    "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
    "            #print(VP)\n",
    "            #print(verb)\n",
    "            \n",
    "            #print(prep_part_tokens)\n",
    "            #[prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
    "            #print(verb)\n",
    "            #print(prep_tokens)\n",
    "            subject = find_subj(parse_tree)\n",
    "            #print(subject)\n",
    "            \n",
    "            #Calc rest of VP for Question Generation\n",
    "            [VP.remove(x) for x in prep_tokens + [rem_verb_phrase]  if x in tok]\n",
    "            #calc rest of the sentence for question generation\n",
    "            #answer = \" \".join(ans_tokens)\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in prep_tokens + VP + [rem_verb_phrase] + subject   if x in tok ]\n",
    "            quest_tok =  [ques]+ [verb] + subject + VP  + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : prep })\n",
    "    return QA\n",
    "\n",
    "#QSG_Rule_2_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSG_Rule_2_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<IN>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\n",
    "        seg_pos = parse_tree.pos()\n",
    "        rule2_2_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        seg = \" \".join([p[0] for p in seg_pos ])\n",
    "        #print(seg)\n",
    "        #print(rule2_2_chunks)\n",
    "        prep_chunk = find_chunk(rule2_2_chunks)\n",
    "        prep_pos,prep_tokens = get_pos_tokens_from_chunk_tree(prep_chunk[0]) if prep_chunk else (None,None)\n",
    "        #print(prep_pos)\n",
    "        #print(prep_tokens)\n",
    "        prep = \" \".join(prep_tokens) if prep_tokens else []\n",
    "        clause_chunk  = find_chunk(chunk)\n",
    "        \n",
    "        if len(clause_chunk) > 1 :\n",
    "            clause_strings = [\" \".join(get_pos_tokens_from_chunk_tree(c)[1]) for c in clause_chunk ]\n",
    "            prep_index = seg.index(seg)\n",
    "            clause_index  = [abs(seg.index(cs)-prep_index)  for cs in clause_strings ]\n",
    "            cl_chunk = clause_chunk[clause_index.index(min(clause_index))]\n",
    "            cl_string = clause_strings[clause_index.index(min(clause_index))].split(\" \")\n",
    "            verb = verb_phrase_identification([findLCA(parse_tree,cl_string[0],cl_string[-1])],[True],[cl_chunk])[0]\n",
    "        else :\n",
    "            cl_chunk = clause_chunk[0] if clause_chunk else None\n",
    "        cl_pos,cl_tokens = get_pos_tokens_from_chunk_tree(cl_chunk)\n",
    "        #print(cl_pos)\n",
    "        #print(cl_tokens)\n",
    "        if prep_chunk :\n",
    "            q_prep = find_chunk(parse_chunks(seg_pos,\"{<IN+>}\"))[0]\n",
    "            q_prep_pos,q_prep_tokens = get_pos_tokens_from_chunk_tree(q_prep)\n",
    "            #print(q_prep_tokens)\n",
    "            ques = \"what\"\n",
    "            qsd1,ques = QSD_Rule_1(prep_pos)\n",
    "            if not qsd1 :\n",
    "                prep_ners = find_ner_tag_for_tokens(ner,prep_tokens)\n",
    "                qsd3,ques = QSD_Rule_3(prep_pos,prep_ners)\n",
    "                ques = ques\n",
    "                if not qsd3 :\n",
    "                    qsd4,ques = QSD_Rule_4(prep_ners,\"QSG_RULE_2_2\")\n",
    "                if not qsd4 :\n",
    "                    ques = QSD_Rule_5(prep_pos,prep_ners)\n",
    "            #print(ques)\n",
    "                \n",
    "            VP = find_VP(parse_tree)\n",
    "            if not VP :\n",
    "                break\n",
    "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
    "            #print(VP)\n",
    "            #print(verb)\n",
    "            \n",
    "            #print(prep_part_tokens)\n",
    "            #[prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
    "            #print(verb)\n",
    "            #print(prep_tokens)\n",
    "            subject = find_subj(parse_tree)\n",
    "            #print(subject)\n",
    "            \n",
    "            #Calc rest of VP for Question Generation\n",
    "            [VP.remove(x) for x in prep_tokens + [rem_verb_phrase]  if x in tok]\n",
    "            #calc rest of the sentence for question generation\n",
    "            #answer = \" \".join(ans_tokens)\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in prep_tokens + VP + [rem_verb_phrase] + subject   if x in tok ]\n",
    "            quest_tok = q_prep_tokens +[ques]+ [verb] + subject + VP  + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : prep })\n",
    "    return QA\n",
    "\n",
    "#QSG_Rule_2_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSG_Rule_2_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
    "    QA = []\n",
    "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
    "    # Counter example for is_cl - \"A house is for $100,00\"\n",
    "        rule_grammar = \"{<VB.?|MD|RP|RB.?>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\n",
    "        seg_pos = parse_tree.pos()\n",
    "        rule2_3_chunks = parse_chunks(seg_pos,rule_grammar)\n",
    "        seg = \" \".join([p[0] for p in seg_pos ])\n",
    "        #print(seg)\n",
    "        #print(rule2_3_chunks)\n",
    "        prep_chunk = find_chunk(rule2_3_chunks)\n",
    "        prep_pos,prep_tokens = get_pos_tokens_from_chunk_tree(prep_chunk[0]) if prep_chunk else (None,None)\n",
    "        #print(prep_pos)\n",
    "        #print(prep_tokens)\n",
    "        prep = \" \".join(prep_tokens) if prep_tokens else []\n",
    "        \n",
    "        #print(cl_pos)\n",
    "        #print(cl_tokens)\n",
    "        if prep_chunk :\n",
    "            ques = \"what\"\n",
    "            two_ques = False\n",
    "            qsd1,ques = QSD_Rule_1(prep_pos)\n",
    "            prep_ners = find_ner_tag_for_tokens(ner,prep_tokens)\n",
    "            qsd2 , prp_tokens , noun_tokens = QSD_Rule_2(parse_tree)\n",
    "            two_ques = qsd2\n",
    "            if not qsd2 :\n",
    "                qsd3,ques = QSD_Rule_3(prep_pos,prep_ners)\n",
    "            #print(ques)\n",
    "            VP = find_VP(parse_tree)\n",
    "            if not VP :\n",
    "                break\n",
    "            #print(VP)\n",
    "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
    "            \n",
    "            #print(verb)\n",
    "            subject = find_subj(parse_tree)\n",
    "            #print(subject)\n",
    "            ans_tokens = noun_tokens\n",
    "            if two_ques :\n",
    "                ques1_ans = prp_tokens\n",
    "                VP_q1 = VP[:]\n",
    "                tok_q1 = tok[:]\n",
    "                [VP_q1.remove(x) for x in ques1_ans  if x in VP_q1]\n",
    "                [tok_q1.remove(x) for x in ques1_ans + VP_q1 + [rem_verb_phrase] + subject   if x in tok_q1 ]\n",
    "                q1_token = [\"Whom\"]+ [verb] + subject + VP_q1  + tok_q1 + [\"?\"]\n",
    "                ques1 = \" \".join(q1_token)\n",
    "                QA.append({\"Q\" : ques1 , \"A\" : \" \".join(ques1_ans)  })\n",
    "           \n",
    "            \n",
    "            \n",
    "            #print(prep_part_tokens)\n",
    "            #[prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
    "            #print(verb)\n",
    "            #print(prep_tokens)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Calc rest of VP for Question Generation\n",
    "            [VP.remove(x) for x in ans_tokens  if x in VP]\n",
    "            #calc rest of the sentence for question generation\n",
    "            answer = \" \".join(ans_tokens)\n",
    "            tok = tok[:]\n",
    "            [tok.remove(x) for x in ans_tokens + VP + [rem_verb_phrase] + subject   if x in tok ]\n",
    "            quest_tok = [ques]+ [verb] + subject + VP  + tok + [\"?\"]\n",
    "            #print(quest_tok)\n",
    "            question = \" \".join(quest_tok)\n",
    "            QA.append({\"Q\" : question , \"A\" : answer  })\n",
    "    return QA\n",
    "\n",
    "#QSG_Rule_2_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = \"Barack Obama is the president of The United States of America.\"\n",
    "#sentence = \"IIIT Hyderabad is the venue of IASNLP-2018.\"\n",
    "#sentence = \"The boy went by bus.\"\n",
    "#sentence =  sentence.rstrip().rstrip(\".\")\n",
    "\n",
    "sentence = \"\"\" Deputy Chief Minister and Services Department Minister-in-charge Manish Sisodia ordered the Department\n",
    "to change the approving authority for transfers of officers from the Lieutenant-Governor and bureaucrats\n",
    "to the Chief Minister and Ministers.\"\"\"\n",
    "#sentence = \"The contractor will build you a house for $100,000 dollars.\"\n",
    "#sentence = \"The book might cost me $10.\"\n",
    "#sentence=\"The book might cost me $10 from the store.\"\n",
    "#sentence = \"$100,000 builds a house out of sticks.\"\n",
    "#sentence = \"The bill will cost them 500 million dollars in India.\"\n",
    "#sentence = \"His name is Robinson.\"\n",
    "#sentence = \"She will quickly pour the sticky liquid into the green flowery pot.\"\n",
    "#sentence = \"I am going quickly back on Saturday.\"\n",
    "#sentence = \"He wants to become a good doctor.\"\n",
    "#sentence = \"I want to work.\"\n",
    "#sentence = \"He hurriedly left the class in the morning.\"\n",
    "#sentence = \"He is addicted to smoking.\"\n",
    "#sentence = \"He will go by bus.\"\n",
    "#sentence = \"John gave Mary a book.\" #design more rules to catch the essence\n",
    "#sentence = \"He gave him a book.\"\n",
    "#sentence = \"He will buy a book.\"\n",
    "#sentence = \"He gave him a book.\"\n",
    "#sentence = \"John gave Mary a book.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{'properties': \"{'annotators': 'pos,parse', 'outputFormat': 'json'}\", 'pipelineLanguage': 'en'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:9000 \"POST /?properties=%7B%27annotators%27%3A+%27pos%2Cparse%27%2C+%27outputFormat%27%3A+%27json%27%7D&pipelineLanguage=en HTTP/1.1\" 200 16395\n",
      "INFO:root:{'properties': \"{'annotators': 'ner', 'outputFormat': 'json'}\", 'pipelineLanguage': 'en'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:9000 \"POST /?properties=%7B%27annotators%27%3A+%27ner%27%2C+%27outputFormat%27%3A+%27json%27%7D&pipelineLanguage=en HTTP/1.1\" 200 6288\n"
     ]
    }
   ],
   "source": [
    "#All preprocessing\n",
    "segments = sentence.rstrip().rstrip(\".\").split(\", \")\n",
    "tree = Tree.fromstring(nlp.parse(sentence))\n",
    "ner = nlp.ner(sentence)\n",
    "tokens = [word_tokenize(segment) for segment in segments]\n",
    "parse_trees = [findLCA(tree,seg[0],seg[-1]) for seg in tokens]\n",
    "ner_split = [list(g) for k,g in itertools.groupby(ner,lambda x: x[0] == ',') if not k]\n",
    "clause_identification_grammar = \"{<DT>?<JJ.?>*<\\$|CD|NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?|VB.?|MD|RP>+}\"\n",
    "chunks = [ parse_chunks(pos_tag(word_tokenize(segment)) ,clause_identification_grammar) for segment in segments ]\n",
    "is_clause_val = is_clause(chunks)\n",
    "\n",
    "new_trees,enrichment_update =  enrich_VPs(parse_trees)\n",
    "parse_trees = new_trees\n",
    "update_indices = indices = [i for i, x in enumerate(enrichment_update) if x == True]\n",
    "\n",
    "for index in update_indices : \n",
    "    tree = parse_trees[index]\n",
    "    #Not changing original segments as they will be used later to form questions\n",
    "    #segments[index] =  \" \".join(tree.leaves())\n",
    "    chunks[index] = parse_chunks(tree.pos() ,clause_identification_grammar)\n",
    "    ner_split[index]  = nlp.ner(\" \".join(tree.leaves()))\n",
    "    tokens[index] = tree.leaves()\n",
    "    #print(ner_split)\n",
    "    #print(tree.pos())\n",
    "\n",
    "verb_phrases = verb_phrase_identification(parse_trees,is_clause_val,chunks)\n",
    "    #all the pre processed data we have\n",
    "#print(segments)\n",
    "#print(chunks)\n",
    "#print(parse_trees)\n",
    "#for ptree in parse_trees :\n",
    "    #ptree.draw()\n",
    "#print(verb_phrases)\n",
    "#print(is_clause_val)\n",
    "#print(tokens)\n",
    "#print(ner_split)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Q': 'who ordered the Department to change the approving authority for transfers of officers from the Lieutenant-Governor and bureaucrats to the Chief Minister and Ministers ?', 'A': 'Deputy Chief Minister and Services Department Minister-in-charge Manish Sisodia'}]\n",
      "[{'Q': 'To what did Deputy Chief Minister and Services Department Minister-in-charge Manish Sisodia Department change the approving authority for transfers of officers from the Lieutenant-Governor and bureaucrats to the and Ministers ?', 'A': 'to the Chief Minister'}]\n",
      "[{'Q': 'for what did Deputy Chief Minister and Services Department Minister-in-charge Manish Sisodia the Department to change the approving authority of officers from the Lieutenant-Governor and bureaucrats to the Chief Minister and Ministers ?', 'A': 'for transfers'}]\n",
      "[{'Q': 'What did Deputy Chief Minister and Services Department Minister-in-charge Manish Sisodia ordered the Department to change the approving authority for transfers of officers from the Lieutenant-Governor and bureaucrats to the Chief Minister and Ministers ?', 'A': ''}]\n",
      "[{'Q': 'What did Deputy Chief Minister and Services Department Minister-in-charge Manish Sisodia ordered Department the for transfers of officers from the Lieutenant-Governor and bureaucrats to the Chief Minister and Ministers to change ?', 'A': 'the approving authority'}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(QSG_Rule_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_2_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_2_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_2_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_2_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_5(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_6_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_6_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_6_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "print(QSG_Rule_7(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
